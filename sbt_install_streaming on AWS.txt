step1
echo "deb https://dl.bintray.com/sbt/debian /" | sudo tee -a /etc/apt/sources.list.d/sbt.list
step2
sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 642AC823
step3
sudo apt-get update
step4
sudo apt-get install sbt

step5
sbt -version

step6
Make a plugin directory inside .sbt/0.13/

mkdir -p .sbt/0.13/plugins

step7

Create a file plugins.sbt

sudo gedit .sbt/0.14/plugins/plugins.sbt

step8

addSbtPlugin("com.typesafe.sbteclipse" % "sbteclipse-plugin" % "4.0.0")

step9
sbt package

mkdir helloworld

cd helloworld/

mkdir -p src/main/scala

sudo gedit src/main/scala/hello.scala

step10

Put the below code.

======================CODE==============================

import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
import org.apache.spark.streaming._


object NetworkWordCount {
  def main(args: Array[String]) {

    val sparkConf = new SparkConf().setAppName("NetworkWordCount").setMaster("local[2]")
    val ssc = new StreamingContext(sparkConf, Seconds(15))

    val lines = ssc.socketTextStream("localhost", 9999)
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
    wordCounts.print()
    ssc.start()
    ssc.awaitTermination()
  }

}
***********************************************************************************************************
Create a build.sbt file inside the helloworld directory.

sudo gedit build.sbt

=============================build.sbt==================================

name := "NetworkWordCount"
version := "1.0"
scalaVersion := "2.11.8"

libraryDependencies += "org.apache.spark" %% "spark-core" % "2.1.0"
libraryDependencies += "org.apache.spark" %% "spark-streaming" % "2.1.0"
************************************************************************************************************
$ sbt compile

$sbt run

sbt package
========================================Terminal1======================================================

spark-submit --class "NetworkWordCount" --master local[2] target/scala-2.10/networkwordcount_2.10-1.0.jar


  In AWS

ubuntu@ip-172-31-4-91:/usr/local/scala/spark-2.0.0-bin-hadoop2.6$ ./bin/spark-submit /home/ubuntu/helloworld/target/scala-2.11/networkwordcount_2.11-1.0.jar
**********************************************************************************************************************

========================================Terminal2==============================================

nc -lk 9999

/*
'''''''''
your words
'''''''*/
*******************************************************************************
========================================Terminal1==========================================================

cntrl+c to stop the streaming and once the streaming is stopped scroll up you will find the wordcount of the strings you have entered in terminal2

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`
jar
https://github.com/pushkar-mishra/twitter-spark-streaming/blob/master/dstream-twitter_2.11-0.1.0-SNAPSHOT.jar

~~~~~~~~~~~~~~~~~~~

import org.apache.spark.streaming.twitter._
import twitter4j.auth._
import twitter4j.conf._
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext._
val ssc = new StreamingContext(spark.sparkContext,Seconds(10))
val cb = new ConfigurationBuilder
cb.setDebugEnabled(true).setOAuthConsumerKey("FuBORP7jbL3vZeBVn3QnEOrHB").setOAuthConsumerSecret("2nD6cYxBmI3zYBJneKg5zMRDHQ5SOJ3KZaG5LrVHPxIbyqyLvV").setOAuthAccessToken("2898611184-qvHbiZKPY4f52DO8nDYmIznVffjTg1LfwhgH0YC").setOAuthAccessTokenSecret("YwY39UxW4lzrnds6Mwyw8781X40szfHQmN6NG4WuRY2lV")
val auth = new OAuthAuthorization(cb.build)
val tweets = TwitterUtils.createStream(ssc,auth)

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

evar

/Import the necessary packages into the Spark Program
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.SparkContext._
...
import java.io.File
 
object twitterSentiment {
 
def main(args: Array[String]) {
if (args.length < 4) {
System.err.println("Usage: TwitterPopularTags <consumer key> <consumer secret> " + "<access token> <access token secret> [<filters>]")
System.exit(1)
}
 
StreamingExamples.setStreamingLogLevels()
//Passing our Twitter keys and tokens as arguments for authorization
val Array(consumerKey, consumerSecret, accessToken, accessTokenSecret) = args.take(4)
val filters = args.takeRight(args.length - 4)
 
// Set the system properties so that Twitter4j library used by twitter stream
// Use them to generate OAuth credentials
System.setProperty("twitter4j.oauth.consumerKey", consumerKey)
...
System.setProperty("twitter4j.oauth.accessTokenSecret", accessTokenSecret)
 
val sparkConf = new SparkConf().setAppName("twitterSentiment").setMaster("local[2]")
val ssc = new Streaming Context
val stream = TwitterUtils.createStream(ssc, None, filters)
 
//Input DStream transformation using flatMap
val tags = stream.flatMap { status => Get Text From The Hashtags }
 
//RDD transformation using sortBy and then map function
tags.countByValue()
.foreachRDD { rdd =>
val now = Get current time of each Tweet
rdd
.sortBy(_._2)
.map(x => (x, now))
//Saving our output at ~/twitter/ directory
.saveAsTextFile(s"~/twitter/$now")
}
 
//DStream transformation using filter and map functions
val tweets = stream.filter {t =>
val tags = t. Split On Spaces .filter(_.startsWith("#")). Convert To Lower Case
tags.exists { x => true }
}
 
val data = tweets.map { status =>
val sentiment = SentimentAnalysisUtils.detectSentiment(status.getText)
val tagss = status.getHashtagEntities.map(_.getText.toLowerCase)
(status.getText, sentiment.toString, tagss.toString())
}
 
data.print()
//Saving our output at ~/ with filenames starting like twitters
data.saveAsTextFiles("~/twitters","20000")
 
ssc.start()
ssc.awaitTermination()
 }
}
~~~~~~~~~~~~~