1. Start the Spark shell:
 $ spark-shell 
2. Do the imports:
scala>import org.apache.spark.ml.classification.LogisticRegression

 scala> import org.apache.spark.ml.linalg.{Vector, Vectors}
 3. Create a tuple for Lebron, who is a basketball player, is 80 inches tall, and weighs 250 lbs: 

scala> val lebron = (1.0,Vectors.dense(80.0,250.0))

 4. Create a tuple for Tim, who is not a basketball player, is 70 inches tall, and weighs 150 lbs: 

scala> val tim = (0.0,Vectors.dense(70.0,150.0))

 5. Create a tuple for Brittany, who is a basketball player, is 80 inches tall, and weighs 207 lbs: 

scala> val brittany = (1.0,Vectors.dense(80.0,207.0))

 6. Create a tuple for Stacey, who is not a basketball player, is 65 inches tall, and weighs 120 lbs:

 scala> val stacey = (0.0,Vectors.dense(65.0,120.0))

7. Create a training DataFrame:

 scala> val training = spark.createDataFrame(Seq        (lebron,tim,brittany,stacey)).toDF("label","features")

 8. Create a LogisticRegression estimator: 

 scala> val estimator = new LogisticRegression

 9. Create a transformer by fitting the estimator with the training DataFrame:  

 scala> val transformer = estimator.fit(training)

 10. Now, let's create a test data—John is 90 inches tall, weighs 270 lbs, and is a basketball player:      

scala> val john = Vectors.dense(90.0,270.0) 

11. Create more test data—Tom is 62 inches tall, weighs 150 lbs, and is not a basketball player: 

scala> val tom = Vectors.dense(62.0,120.0) 

12. Create a test data DataFrame: 
  scala> val test=spark.createDataFrame(Seq( (1.0, john), (0.0,tom)        )).toDF("label","features") 


13. Do the prediction using the transformer: 


  scala> val results = transformer.transform(test) 


14. Print the schema of the results DataFrame:            scala> results.printSchema       
 root        |-- label: double (nullable = false)        |-- features: vector (nullable = true)        |-- rawPrediction: vector (nullable = true)        |-- probability: vector (nullable = true)
        |-- prediction: double (nullable = true)
 As you can see, besides prediction, the transformer has also added rawPrediction and a probability column. 

15. Print the DataFrame results:

 scala> results.show        +-----+------------+----------------+--------------------+-------+        |label| features| rawPrediction| probability|prediction|        +-----+------------+----------------+--------------------+-------+        | 1.0|[90.0,270.0]|[-61.884758625897...|[1.32981373684616...| 1.0|        | 0.0|[62.0,120.0]|[31.4607691062275...|[0.99999999999997...| 0.0|        +-----+------------+----------------+--------------------+-------+

 16. Let's select only features and prediction:


 scala> val predictions = results.select          ("features","prediction").show

        +------------+----------+        | features|prediction|        +------------+----------+        |[90.0,270.0]| 1.0|        |[62.0,120.0]| 0.0|        +------------+----------+



import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.sql.SparkSession
import org.apache.spark.ml.linalg.Vectors
object ML_example {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .master("local[*]")
      .appName("myfirstlogistic")
      .config("spark.sql.warehouse.dir", ".")
      .getOrCreate()
    val x = Vectors.dense(380.0, 3.61, 3.0)
    val trainingdata=Seq(
      (0.0, Vectors.dense(380.0, 3.61, 3.0)),
      (1.0, Vectors.dense(660.0, 3.67, 3.0)),
      (1.0, Vectors.dense(800.0, 1.3, 1.0)),
      (1.0, Vectors.dense(640.0, 3.19, 4.0)),
      (0.0, Vectors.dense(520.0, 2.93, 4.0)),
      (1.0, Vectors.dense(760.0, 3.00, 2.0)),
      (1.0, Vectors.dense(560.0, 2.98, 1.0)),
      (0.0, Vectors.dense(400.0, 3.08, 2.0)),
      (1.0, Vectors.dense(540.0, 3.39, 3.0)),
      (0.0, Vectors.dense(700.0, 3.92, 2.0)),
      (0.0, Vectors.dense(800.0, 4.0, 4.0)),
      (0.0, Vectors.dense(440.0, 3.22, 1.0)),
      (1.0, Vectors.dense(760.0, 4.0, 1.0)),
      (0.0, Vectors.dense(700.0, 3.08, 2.0)),
      (1.0, Vectors.dense(700.0, 4.0, 1.0)),
      (0.0, Vectors.dense(480.0, 3.44, 3.0)),
      (0.0, Vectors.dense(780.0, 3.87, 4.0)),
      (0.0, Vectors.dense(360.0, 2.56, 3.0)),
      (0.0, Vectors.dense(800.0, 3.75, 2.0)),
      (1.0, Vectors.dense(540.0, 3.81, 1.0))
    )

    val trainingDF = spark.createDataFrame(trainingdata).toDF("label", "features")
    trainingDF.show()
    trainingDF.printSchema()
    val lr_Estimator = new LogisticRegression().setMaxIter(80).setRegParam(0.01).setFitIntercept(true)
    val Admission_lr_Model = lr_Estimator.fit(trainingDF)
    println("Admission_lr_Model parameters:")
    println(Admission_lr_Model.parent.extractParamMap)
    println("Admission_lr_Model Summary:")
    println(Admission_lr_Model.summary.predictions)
    val predict=Admission_lr_Model.transform(trainingDF)
    predict.printSchema()
    val label1=predict.select("label").collect()
    val features1=predict.select("features").collect()
    val probability=predict.select("probability").collect()
    val prediction=predict.select("prediction").collect()
    val rawPrediction=predict.select("rawPrediction").collect()
    println("Training Set Size=", label1.size )

  }
}

~~~~~~~~~~~~~~~~

K-means

import org.apache.log4j.{Level, Logger}
import org.apache.spark.ml.clustering.KMeans
import org.apache.spark.sql.SparkSession


object MyKMeansCluster {

  def main(args: Array[String]): Unit = {


    Logger.getLogger("org").setLevel(Level.ERROR)

    // setup SparkSession to use for interactions with Spark
    val spark = SparkSession
      .builder
      .master("local[*]")
      .appName("myKMeansCluster")
      .config("spark.sql.warehouse.dir",  ".")
      .getOrCreate()


    val trainingData = spark.read.format("libsvm").load("../data/sparkml2/chapter8/my_kmeans_data.txt")

    trainingData.show()


    // Trains a k-means model
    val kmeans = new KMeans()
      .setK(3)   // default value is 2
      .setFeaturesCol("features")
      .setMaxIter(10)   // default Max Iteration is 20
      .setPredictionCol("prediction")
      .setSeed(1L)
    val model = kmeans.fit(trainingData)

    model.summary.predictions.show()
    // the fit function will run the algo and do calculation, it is based on DataFrame created above.

    println("KMeans Cost:" +model.computeCost(trainingData))   //Within Set Sum of Squared Error (WSSSE)

    println("KMeans Cluster Centers: ")
    model.clusterCenters.foreach(println)
    spark.stop()
  }
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

object Com {
  private var a: Int = 34
  def show2(): Unit ={
    var d = new Com()
    println("hello sir:"+d.c)
  }
}
class Com {

  import Com._
private var c:String = "hadoop"
  def show() {
    println("hello evarcity:" + a)
  }
}
object G1{
  def main(args: Array[String]): Unit = {
    var b = new Com()
    b.show()
    Com.show2()

  }

  }
~~~~~~~~~~~~~~~~
why is compainon object

object Hadoop {
  private var a: Int = 87

  def show(): Unit = {
    println(a)
    var b1 = new H2()
    println("hi:"+b1.b)

  }
}
class H2 {
  private var b: String = "hello"

  def show1(): Unit = {
  }
}
object J{
  def main(args: Array[String]): Unit = {
    var n = new H2()
    n.show1()
    Hadoop.show()
  }
  }

~~~~~~~~~~~~~~~~`
Constructor

As in Java or C++, a Scala class can have as many constructors as you like. However, a Scala class has one constructor that is more important than all the others, called the primary constructor. In addition, a class may have any number of auxiliary constructors.

We discuss auxiliary constructors first because they are easier to understand. They are similar to constructors in Java or C++, with just two differences.

The auxiliary constructors are called this. (In Java or C++, constructors have the same name as the class—which is not so convenient if you rename the class.)

Each auxiliary constructor must start with a call to a previously defined auxiliary constructor or the primary constructor.

Here is a class with two auxiliary constructors:

class Person {
  private var name = ""
  private var age = 0

  def this(name: String) { // An auxiliary constructor
    this() // Calls primary constructor
    this.name = name
  }

  def this(name: String, age: Int) { // Another auxiliary constructor
    this(name) // Calls previous auxiliary constructor
    this.age = age
  }
}
We will look at the primary constructor in the next section. For now, it is sufficient to know that a class for which you don’t define a primary constructor has a primary constructor with no arguments.

You can construct objects of this class in three ways:

val p1 = new Person // Primary constructor
val p2 = new Person("Fred") // First auxiliary constructor
val p3 = new Person("Fred", 42) // Second auxiliary constructor

~~~~~~~~~~~~~~~~

5.7 The Primary Constructor
In Scala, every class has a primary constructor. The primary constructor is not defined with a this method. Instead, it is interwoven with the class definition.

The parameters of the primary constructor are placed immediately after the class name.

class Person(val name: String, val age: Int) {
  // Parameters of primary constructor in (...)
  ...
}
Parameters of the primary constructor turn into fields that are initialized with the construction parameters. In our example, name and age become fields of the Person class. A constructor call such as new Person("Fred", 42) sets the name and age fields.

Half a line of Scala is the equivalent of seven lines of Java:

public class Person { // This is Java
  private String name; private int age; public Person(String name, int age) {
    this.name = name; this.age = age;
  }
  public String name() { return this.name; } public int age() { return this.age; }
  ...
}
The primary constructor executes all statements in the class definition. For example, in the following class

class Person(val name: String, val age: Int) {
  println("Just constructed another person")
  def description = s"$name is $age years old"
}
the println statement is a part of the primary constructor. It is executed whenever an object is constructed.

This is useful when you need to configure a field during construction. For example:

class MyProg {
  private val props = new Properties
  props.load(new FileReader("myprog.properties"))
    // The statement above is a part of the primary constructor
  ...
}

~~~~~~~~~~~~~~`